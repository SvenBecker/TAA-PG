üåç
*[English](README.md) ‚àô [German](README_de.md)*

# Reinforcement Learning f√ºr die taktische Asset Allocation
> Dieses Projekt beinhaltet das Trainieren und das Testen verschiedener RL Agenten auf einem Portfolio Environment.
> Hierbei gilt f√ºr die Interaktion zwischen Environment und Agent: \
> Environment <> Runner <> Agent <> Model

## Wichtige Dateien und Ordner

- Datenbeschaffung: [data.py](env/data/data.py)
- Environment: [environment.py](env/environment.py)
- Hyperparametertuning des Prognosemodells: [predictor.py](/env/predictor.py)
- Training des Prognosemodells: [predictor_network.py](env/predictor_network.py)
- Runner: [runner.py](run/runner.py)
- Grundlegende Konfigurationsparameter: [config.py](config.py)
- Training: [train.py](run/train.py)
- Test: [test.py](run/test.py)
- Agenten Konfigurationen: [agent_config](model/agent_config/)
- NN Konfiguration: [net_config](model/net_config/) 

### Voraussetzungen

Die hier vorgestellte Implementierung erfolgte haupts√§chlich in Python,
daher wird zur Replikation der Ergebnisse oder dem Testen der Implementierung 
Python Version >= 3.6 vorausgesetzt.
Folgende Python Packages werden zudem ben√∂tigt:

- h5py==2.7.1
- Keras==2.1.3
- matplotlib==2.1.0
- numpy==1.14.1
- pandas==0.20.3
- pandas-datareader==0.5.0
- scikit-learn==0.19.1
- scipy==1.0.0
- seaborn==0.8.1
- tensorflow==1.4.0
- tensorflow-tensorboard==0.4.0rc3
- tensorforce==0.3.5.1

## Ausf√ºhren der Trainingsdatei

Zum Training eines Agenten sollte die Datei [train.py](run/train.py) √ºber die Konsole ausgef√ºhrt werden. \
Zum Beispiel:
```
python ~/path/to/file/run/train.py -at "clipping" -v 1
```

√Ñnderungen an den Environment und Run Parametern k√∂nnen innerhalb der [Trainingsdatei](run/train.py),
der [Config Datei](config.py) oder √ºber den nachfolgend vorgestellten Flags vorgenommen werden. 

Anpassungen der [Agenten](model/agent_config) und [Modell](model/net_config) Spezifikationen m√ºssen √ºber die jeweiligen 
Konfigurationsdatein im json-Format vorgenommen werden. 

### Flags:

| Flag 1 | Flag 2 | Bedeutung |
|:----:|:----:|-----------|
| -d | --data | Pfad der environment.csv Datei |
| -sp | --split | Training/Test Split |
| -th | --threaded | (bool) Threaded Runner oder Single Runner |
| -ac | --agent-config | Pfad der Agenten Configdatei |
| -nw | --num-worker | Anzahl an Threads falls Threaded ausgew√§hlt wurde |
| -ep | --epochs | Anzahl an Epochen |
| -e | --episodes | Anzahl an Epsioden |
| -hz | --horizon | Investmenthorizont |
| -at | --action-type | Aktionstyp: 'signal', 'signal_softmax', 'direct', 'direct_softmax', 'clipping' |
| -as | --action-space | Aktionsraum: 'unbounded', 'bounded', 'discrete' |
| -na | --num-actions | Anzahl an Aktionen bei diskretem Aktionsraum |
| -mp | --model-path | Pfad f√ºr das Speichern des Agenten |
| --eph | --eval-path | Pfad f√ºr das Speichern der Evaluierungsdateien |
| -v | --verbose | Gibt an wie viel in der Konsole agezeigt werden soll |
| -l | --load-agent | Falls angegeben wird der Agent von einem fr√ºheren Speicherpunkt geladen (Pfad)|
| -ds | --discrete_states | Diskretisiert den State falls True |
| -ss | -standardize-state | (bool) Standardisierter oder normaliserter State |
| -rs | --random-starts | Entweder je Epoche die gleiche Reihenfolge an Episodenstarts oder zuf√§llige |

## Ausf√ºhren des Testdatei

Das Ausf√ºhren der [Testdatei](run/test.py) entspricht im Wesentlichen dem der Trainingsdatei. 
Es sollte unter [saves](model/saves) ein Checkpoint f√ºr das Testen des ausgew√§hlten Agenten vorliegen.
Grunds√§tzlich werden automatisch die Modelle der trainierten Agenten unter der jeweiligen
Agentenbezeichnung in diesem [Ordner](model/saves) gespeichert. 
```
python ~/path/to/project/run/test.py -l /project/model/saves/AgentName
```

Der Ordner [saved_results](saved_results) enth√§lt f√ºr mehrere Parameterkonstellationen
bereits vortrainierte Agenten. Um diese zu testen, m√ºssen dementsprechend die nachfolgenden Flags
passend konfiguriert werden.

### Flags

| Flag | Flag 2 | Bedeutung |
|:----:|:----:|-----------|
| -d | --data | Pfad der environment.csv Datei |
| -ba | --basic-agent | Auswahl eines [BasicAgenten](model/basic_agents.py): 'BuyAndHoldAgent', 'RandomActionAgent'
| -sp | --split | Training/Test Split |
| -ac | --agent-config | Pfad der Agenten Configdatei |
| -e | --episodes | Anzahl an Epsioden |
| -hz | --horizon | Investmenthorizont |
| -at | --action-type | Aktionstyp: 'signal', 'signal_softmax', 'direct', 'direct_softmax', 'clipping' |
| -as | --action-space | Aktionsraum: 'unbounded', 'bounded', 'discrete' |
| -na | --num-actions | Anzahl an Aktionen bei diskretem Aktionsraum |
| --eph | --eval-path | Pfad f√ºr das Speichern der Evaluierungsdateien |
| -v | --verbose | Gibt an wie viel in der Konsole angezeigt werden soll |
| -l | --load-agent | L√§dt den Agenten von einem fr√ºheren Speicherpunkt (Pfad) |
| -ds | --discrete_states | (bool) Diskretisiert den State falls True |
| -ss | -standardize-state | Standardisierter oder normaliserter State |

## TensorBoard

Die Datein [predictor.py](env/predictor.py), sowie [train.py](run/train.py) integrieren
[TensorBoard](https://github.com/tensorflow/tensorboard). 
TensorBoard l√§sst sich √ºber
```
tensorboard --logdir path/to/project/env/board
tensorboard --logdir path/to/project/run/board
```

starten und √ºber localhost:6006 betrachten. 
F√ºr den TRPO Agenten war leider eine Integration von TensorBoard nicht m√∂glich.

## Anmerkung

Ende Februar 2018 gab es eine gr√∂√üeres [TensorForce](https://github.com/reinforceio/tensorforce)
Update. Hier wird noch eine alte Version verwendet und eine Umstellung war zeitlich nicht 
mehr m√∂glich. Es kann somit bei Verwendung einer neueren Tensorforce Version eventuell zu 
Konflikten kommen.

## Experimentelle Details und Ergebnisse

### Historische Daten 
F√ºr die Experimente wurden Exchange Traded Fund (ETF) Portfolios erstellt. Informationen bez√ºglich der 
Preisentwicklungen und der Korrelationen der ETFs k√∂nnen den nachfolgenden Abbildungen entnommen werden.

##### Wertentwicklung der einzelnen Anlagen 
![Anlagen Preise](img/asset_data.png)
##### ETF Preisentwicklungs-Korrelationen
![Anlagen Heatmap](img/asset_heatmap.png)

### Testresultate
Die hier vorgestellten Ergebnisse beziehen sich auf unterschiedlichste Agentenkonfigurationen. Die *Buy-and-Hold* 
Strategie (BaH) stellt den Benchmark dar. 
##### Durchschnittlicher Portfolioreturn bei diskreter Aktionswahl
![value_diskret](img/value_diskret.png)
##### Durchschnittlicher Portfolioreturn bei stetiger Aktionswahl
![value_stetig](img/value_stetig.png)

### Trainingsresultate
Im Nachfolgenden sind zur Veranschaulichung die Trainingserfolge der am besten performenden Agenten abgebildet.  

##### Durchschnittlicher Portfolioreturn bei diskreter Aktionswahl
![lineplots_diskret](img/lineplots_diskret.png)
##### Durchschnittlicher Portfolioreturn bei stetiger Aktionswahl
![lineplots_stetig](img/lineplots_stetig.png)
